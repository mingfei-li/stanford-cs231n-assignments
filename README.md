# Stanford CS231n (Deep Learning for Computer Vision) Assignment Solutions

My solutions to [Stanford CS231n (Deep Learning for Computer Vision)](https://cs231n.stanford.edu/2024/) assignments.

| Assignment | Algorithms Implemented |
| ---------- | -------------- |
| 1. Traditional ML-based Classifiers: [kNN](assignment1/knn.ipynb), [SVM](assignment1/svm.ipynb), [Softmax](assignment1/softmax.ipynb) |  kNN, SVM, and Softmax classifiers |
| 2. MLP from Scratch: [Two-layer](assignment1/two_layer_net.ipynb), [Multi-layer](assignment2/FullyConnectedNets.ipynb) | NumPy implementation of Linear / ReLU layers, Softmax/SVM losses, Optimizers (SGD w/ momentu, RMSProp, Adam) |
| 3. Training techniques: [Normalization](assignment2/BatchNormalization.ipynb), [Regularization](assignment2/Dropout.ipynb) | NumPy implementation of BatchNorm, LayerNorm, Dropout layers|
| 4. Computer Vision: ConvNet ([NumPy](assignment2/ConvolutionalNetworks.ipynb), [PyTorch](assignment2/PyTorch.ipynb)), [Feature Engineering](assignment1/features.ipynb) | NumPy implementation of Convolution, MaxPooling, SpatialBatchNorm, SpatialGroupNorm layers; PyTorch implementation of ConvNets|
| 5. Multimodal sequence models: [Vanilla RNN](assignment3/RNN_Captioning.ipynb), [LSTM](assignment3/LSTM_Captioning.ipynb), [Transformer](assignment3/Transformer_Captioning.ipynb) | NumPy implementation of embedding, RNN, and LSTM modules; PyTorch implementation of Transformer |
| 6. Unsupervised Learning: [GAN](assignment3/Generative_Adversarial_Networks.ipynb), [Contrastive Learning](assignment3/Self_Supervised_Learning.ipynb) | Vanilla GAN, LS-GAN, SimCLR |